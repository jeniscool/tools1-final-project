{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools 1 Final Project\n",
    "\n",
    "Jen Lee, Isaac Burmingham, & Dan Saubert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insert Dataset and motivation slide (1 points)**\n",
    "- How/why the dataset was collected and a description of the metadata of your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insert Actual task definition/research question (2 points)**\n",
    "- What real-world problem are you trying to solve? What are the input and output of your "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insert Literature review (2 points)**\n",
    "- What other work has been done in this area, and how is your work novel compared to others?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Gathering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for data gathering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "- Data cleaning and type conversion activity. Please share anything unusual you faced during this activity.\n",
    " - What did you do about missing values and why? Handling missing values properly is very important.\n",
    "- New feature/attribute creation and data summary statistics and interpretation.\n",
    "\n",
    "\n",
    "**for ML analysis to work cleaned dataframe should have columns for 'post' & 'subreddit'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I love politics!</td>\n",
       "      <td>r/politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Biden is bad</td>\n",
       "      <td>r/politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Biden is good</td>\n",
       "      <td>r/politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump is bad</td>\n",
       "      <td>r/politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trump is good</td>\n",
       "      <td>r/politics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               post   subreddit\n",
       "0  I love politics!  r/politics\n",
       "1      Biden is bad  r/politics\n",
       "2     Biden is good  r/politics\n",
       "3      Trump is bad  r/politics\n",
       "4     Trump is good  r/politics"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For now mkaing a dummy-dataframe\n",
    "# pls don't make fun of my posts\n",
    "# Note: more columns can be added these are just the base two we need\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(columns = ['post', 'subreddit'])\n",
    "\n",
    "df = df.append(pd.Series([\"I love politics!\", \"r/politics\"], index=df.columns), ignore_index=True)\n",
    "df = df.append(pd.Series([\"Biden is bad\", \"r/politics\"], index=df.columns), ignore_index=True)\n",
    "df = df.append(pd.Series([\"Biden is good\", \"r/politics\"], index=df.columns), ignore_index=True)\n",
    "df = df.append(pd.Series([\"Trump is bad\", \"r/politics\"], index=df.columns), ignore_index=True)\n",
    "df = df.append(pd.Series([\"Trump is good\", \"r/politics\"], index=df.columns), ignore_index=True)\n",
    "df = df.append(pd.Series([\"Harris is bad\", \"r/politics\"], index=df.columns), ignore_index=True)\n",
    "df = df.append(pd.Series([\"Harris is good\", \"r/politics\"], index=df.columns), ignore_index=True)\n",
    "df = df.append(pd.Series([\"The government is terrible\", \"r/politics\"], index=df.columns), ignore_index=True)\n",
    "df = df.append(pd.Series([\"The White House is White\", \"r/politics\"], index=df.columns), ignore_index=True)\n",
    "\n",
    "df = df.append(pd.Series([\"funny meme lol rawr\", \"r/funny\"], index=df.columns), ignore_index=True)\n",
    "df = df.append(pd.Series([\"i can has cheezburger\", \"r/funny\"], index=df.columns), ignore_index=True)\n",
    "df = df.append(pd.Series([\"shit-post funny lol\", \"r/funny\"], index=df.columns), ignore_index=True)\n",
    "df = df.append(pd.Series([\"lawl comics me_irl\", \"r/funny\"], index=df.columns), ignore_index=True)\n",
    "df = df.append(pd.Series([\"this was funnier the first time i saw it\", \"r/funny\"], index=df.columns), ignore_index=True)\n",
    "df = df.append(pd.Series([\"screen-shot of tweet\", \"r/funny\"], index=df.columns), ignore_index=True)\n",
    "df = df.append(pd.Series([\"look at this idiot\", \"r/funny\"], index=df.columns), ignore_index=True)\n",
    "df = df.append(pd.Series([\"white screen with black background\", \"r/funny\"], index=df.columns), ignore_index=True)\n",
    "df = df.append(pd.Series([\"hahahahaha omg laughter hahahaah\", \"r/funny\"], index=df.columns), ignore_index=True)\n",
    "\n",
    "df.head() # print head to show success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for data cleaning\n",
    "\n",
    "import re\n",
    "\n",
    "# proprocessor function (~should~) put post in lower-case plain-text with no symbols/punctuations\n",
    "# Note: We might not want to use this actually but adding it anyway\n",
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',\n",
    "                           text)\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) +\n",
    "            ' '.join(emoticons).replace('-', ''))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply preprocessor and map classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i love politics</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>biden is bad</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>biden is good</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>trump is bad</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>trump is good</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               post  subreddit\n",
       "0  i love politics           1\n",
       "1      biden is bad          1\n",
       "2     biden is good          1\n",
       "3      trump is bad          1\n",
       "4     trump is good          1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "df['post'] = df['post'].apply(preprocessor)\n",
    "\n",
    "# convert classes (subreddit) to integers \n",
    "class_mapping = {label: idx for idx, label in enumerate(np.unique(df['subreddit']))}\n",
    "class_mapping\n",
    "df['subreddit'] = df['subreddit'].map(class_mapping)\n",
    "\n",
    "# TODO: Insert int-to-subreddit mappings\n",
    "\n",
    "# Print head to show success\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Train Split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# TODO: Change line based on format of the dataframe\n",
    "X, y = df.iloc[:, 0].values, df.iloc[:, 1].values\n",
    "\n",
    "X_train, X_test, y_train, y_test =\\\n",
    "    train_test_split(X, y, \n",
    "                     test_size=0.3, \n",
    "                     random_state=0, \n",
    "                     stratify=y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create functions required to tokenize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    \n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',\n",
    "                           text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) \\\n",
    "                   + ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized\n",
    "\n",
    "def tokenizer_porter(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    \n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',\n",
    "                           text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) \\\n",
    "                   + ' '.join(emoticons).replace('-', '')\n",
    "    \n",
    "    tokenized = [porter.stem(word) for word in text.split()]\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a TfidfVectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=False,\n",
    "                        preprocessor=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Optimization and Serialization\n",
    "\n",
    "Grid search to find the optimal hyperparameters (including choice of stemming algorithm for TfidfVectorizer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "param_grid = [{'vect__ngram_range': [(1, 1), (3, 3), (1, 4)],\n",
    "               'vect__stop_words': [stop, None],\n",
    "               'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "               'clf__penalty': ['l1', 'l2']},\n",
    "              {'vect__ngram_range': [(1, 1), (3, 3), (1, 4)],\n",
    "               'vect__stop_words': [stop, None],\n",
    "               'vect__tokenizer': [tokenizer, tokenizer_porter], \n",
    "               'clf__penalty': ['l1', 'l2'],\n",
    "               'clf__class_weight': ['balanced', None],\n",
    "               'clf__alpha': (1.0000000000000001e-05, 9.9999999999999995e-07)},\n",
    "              ]\n",
    "\n",
    "\n",
    "sgd_tfidf = Pipeline([('vect', tfidf),\n",
    "                     ('clf', SGDClassifier(max_iter=1000, tol=1e-3, loss='log'))])\n",
    "\n",
    "gs_sgd_tfidf = GridSearchCV(sgd_tfidf, param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           cv=5,\n",
    "                           verbose=2,\n",
    "                           n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 120 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=-1)]: Done 276 tasks      | elapsed:    6.9s\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:    7.7s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('vect',\n",
       "                                        TfidfVectorizer(lowercase=False)),\n",
       "                                       ('clf', SGDClassifier(loss='log'))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid=[{'clf__penalty': ['l1', 'l2'],\n",
       "                          'vect__ngram_range': [(1, 1), (3, 3), (1, 4)],\n",
       "                          'vect__stop_words': [['i', 'me', 'my', 'myself', 'we',\n",
       "                                                'our', 'ours', 'ourselves',\n",
       "                                                'you', \"you're\", \"you've\",\n",
       "                                                \"you'll\", \"you'd\", 'your',\n",
       "                                                'yours', 'yoursel...\n",
       "                          'vect__stop_words': [['i', 'me', 'my', 'myself', 'we',\n",
       "                                                'our', 'ours', 'ourselves',\n",
       "                                                'you', \"you're\", \"you've\",\n",
       "                                                \"you'll\", \"you'd\", 'your',\n",
       "                                                'yours', 'yourself',\n",
       "                                                'yourselves', 'he', 'him',\n",
       "                                                'his', 'himself', 'she',\n",
       "                                                \"she's\", 'her', 'hers',\n",
       "                                                'herself', 'it', \"it's\", 'its',\n",
       "                                                'itself', ...],\n",
       "                                               None],\n",
       "                          'vect__tokenizer': [<function tokenizer at 0x7ff57d6f1320>,\n",
       "                                              <function tokenizer_porter at 0x7ff57d6f1680>]}],\n",
       "             scoring='accuracy', verbose=2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_sgd_tfidf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter set: {'clf__alpha': 1e-05, 'clf__class_weight': 'balanced', 'clf__penalty': 'l1', 'vect__ngram_range': (1, 4), 'vect__stop_words': None, 'vect__tokenizer': <function tokenizer_porter at 0x7ff57d6f1680>} \n",
      "\n",
      "CV Accuracy: 0.933\n"
     ]
    }
   ],
   "source": [
    "print('Best parameter set: %s ' % gs_sgd_tfidf.best_params_)\n",
    "print('\\nCV Accuracy: %.3f' % gs_sgd_tfidf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 1.000\n"
     ]
    }
   ],
   "source": [
    "clf = gs_sgd_tfidf.best_estimator_\n",
    "print('Test Accuracy: %.3f' % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the classifier against entire dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# Using paramaters found in grid search\n",
    "clf = SGDClassifier(loss='log', max_iter=1000, tol=1e-3, penalty='l2')\n",
    "\n",
    "vect = HashingVectorizer(decode_error='ignore', \n",
    "                         n_features=2**21,\n",
    "                         preprocessor=None, \n",
    "                         tokenizer=tokenizer)\n",
    "\n",
    "X = vect.transform(X)\n",
    "clf = clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations \n",
    "\n",
    "- Data visualization activity (box plot, bar plot, violin plot, and pairplot to see relationships and distribution, etc.).\n",
    "- Describe anything you find in the data after each visualization.\n",
    "- What data visualization helped you understand about data distribution.\n",
    "- What you did about possible outlier as per data distribution visualization. (Did you confirm with your client whether it is actually an outlier or put a disclosure statement in your notebook if you decided to remove it?)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
